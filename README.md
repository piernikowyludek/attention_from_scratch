# attention_from_scratch
My (long overdue!) repository to help me understand different types of attention in Transformers by implementing them on a lower level.

[x] basic self-attention
[x] basic cross-attention
[x] multi-head attention
[] Linformer
[] Performer
[] Flash Attention
[] MemFormer
[] Masked Attention - example of usage
[] location-aware attention

soft-attention, hard-attention, self-attention, global-attention, local-attention